'''Created on 2016/11/22@author: baoruiha'''from sklearn.utils import shufflefrom sklearn.cross_validation import train_test_splitfrom sklearn.metrics import f1_scorefrom sklearn.datasets import fetch_mldatafrom theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreamsfrom collections import OrderedDictimport numpy as npimport theanoimport theano.tensor as Tfrom theano.ifelse import ifelsetrng = RandomStreams(42)rng = np.random.RandomState(1234)# Multi Layer Perceptronmnist = fetch_mldata('MNIST original', data_home="..")mnist_X, mnist_y = shuffle(mnist.data.astype('float32'),                           mnist.target.astype('int32'), random_state=42)mnist_X = mnist_X / 255.0train_X, valid_X, train_y, valid_y = train_test_split(mnist_X, mnist_y,                                                      test_size=0.2,                                                      random_state=42)train_y = np.eye(10)[train_y]valid_y = np.eye(10)[valid_y]# train_X, valid_X, train_y, valid_y = train_test_split(train_X, train_y,#                                                      test_size=0.2,#                                                      random_state=42)print train_X.shape# Multi Layer Perceptronclass Layer:    # Constructor    def __init__(self, in_dim, out_dim, function):        self.in_dim = in_dim        self.out_dim = out_dim        self.function = function        self.W = theano.shared(rng.uniform(low=-0.08, high=0.08,                                           size=(in_dim, out_dim)                                           ).astype('float32'), name='W')        self.b = theano.shared(np.zeros(out_dim).astype('float32'), name='b')        self.params = [self.W, self.b]    # Forward Propagation    def f_prop(self, x):        self.z = self.function(T.dot(x, self.W) + self.b)        return self.z# Stochastic Gradient Descentdef sgd(params, g_params, eps=np.float32(0.1)):    updates = OrderedDict()    for param, g_param in zip(params, g_params):        updates[param] = param - eps * g_param    return updatesmom = 0.9squared_filter_length_limit = 15.0def sgd_mom(params, g_params, eps=np.float32(0.1)):    gparams_mom = []    for param in params:        gparam_mom = theano.shared(np.zeros(param.get_value(borrow=True).shape,                                            dtype=theano.config.floatX))        gparams_mom.append(gparam_mom)    updates = OrderedDict()    for gparam_mom, gparam in zip(gparams_mom, g_params):        # Misha Denil's original version        updates[gparam_mom] = mom * gparam_mom + (1. - mom) * gparam        # change the update rule to match Hinton's dropout paper    for param, gparam_mom in zip(params, gparams_mom):        # Misha Denil's original version        stepped_param = param - eps * updates[gparam_mom]        # This is a silly hack to constrain the norms of the rows of the weight        # matrices.  This just checks if there are two dimensions to the        # parameter and constrains it if so... maybe this is a bit silly but it        # should work for now.        if param.get_value(borrow=True).ndim == 2:            # constrain the norms of the COLUMNs of the weight, according to            # https://github.com/BVLC/caffe/issues/109            col_norms = T.sqrt(T.sum(T.sqr(stepped_param), axis=0))            desired_norms = T.clip(                col_norms, 0, T.sqrt(squared_filter_length_limit))            scale = desired_norms / (1e-7 + col_norms)            updates[param] = stepped_param * scale        else:            updates[param] = stepped_param    return updateslayers = [    Layer(784, 1000, T.nnet.sigmoid),    Layer(1000, 1000, T.nnet.sigmoid),    Layer(1000, 10, T.nnet.softmax)]x = T.fmatrix('x')t = T.imatrix('t')params = []for i, layer in enumerate(layers):    params += layer.params    if i == 0:        layer_out = layer.f_prop(x)    else:        layer_out = layer.f_prop(layer_out)y = layers[-1].zcost = T.mean(T.nnet.categorical_crossentropy(y, t))g_params = T.grad(cost=cost, wrt=params)updates = sgd_mom(params, g_params)train = theano.function(inputs=[x, t], outputs=cost, updates=updates,                        allow_input_downcast=True, name='train')valid = theano.function(inputs=[x, t], outputs=[cost, T.argmax(y, axis=1)],                        allow_input_downcast=True, name='valid')test = theano.function(inputs=[x], outputs=T.argmax(y, axis=1), name='test')batch_size = 100n_batches = train_X.shape[0] // batch_sizefor epoch in range(100):    train_X, train_y = shuffle(train_X, train_y)    for i in range(n_batches):        start = i * batch_size        end = start + batch_size        train(train_X[start:end], train_y[start:end])    valid_cost, pred_y = valid(valid_X, valid_y)    print('EPOCH:: %i, Validation cost: %.3f, Validation F1: %.3f' %          (epoch + 1, valid_cost,           f1_score(np.argmax(valid_y, axis=1).astype('int32'),                    pred_y, average='macro')))# if __name__ == '__main__':    # datasets = load_mnist()
